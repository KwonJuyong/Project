# app/services/ai_feedback.py

from __future__ import annotations
import os
import re
import json
from typing import List, Dict, Any, Optional, Literal, Tuple

from dotenv import load_dotenv
from openai import AsyncOpenAI
from openai import RateLimitError, APIError, APITimeoutError

load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
OPENAI_MODEL = os.getenv("OPENAI_MODEL", "gpt-4o-mini")  # í†µì¼ëœ ê¸°ë³¸ ëª¨ë¸


class AIFeedbackService:
    """
    ë¬¸ì œìœ í˜•ë³„ AI í”¼ë“œë°± + ì ìˆ˜ ì‚°ì¶œ ì„œë¹„ìŠ¤

    ë°˜í™˜ í˜•ì‹(dict) â€” í•˜ìœ„í˜¸í™˜ ìœ ì§€ + í™•ì¥:
    {
        "percent": float,                 # 0~100 (ë°±ë¶„ìœ¨)
        "score": float,                   # 0~max_points (ìŠ¤ì¼€ì¼ë§, = total_scoreì— ëŒ€ì‘)
        "ai_feedback": str | None,        # í•œêµ­ì–´ í”¼ë“œë°±
        "graded_by": str,                 # coding/debugging: 'auto:testcase', mc/short: 'auto:rule'/'auto:exact', subjective: 'AI'

        # === ì¶”ê°€ í•„ë“œ(ì˜µì…˜): ì¡°ê±´ ì—°ë™ ì¶œë ¥ ===
        "condition_results": [            # DB JSONB(Submission.condition_check_results)ì— ê·¸ëŒ€ë¡œ ë„£ì–´ë„ ë˜ëŠ” í˜•íƒœ
            {
                "condition_id": int,
                "condition": str,
                "status": "pass" | "fail",
                "description": str,
                "feedback": str,
                "score": float,          # í•´ë‹¹ ì¡°ê±´ ë“ì (í†µê³¼ ì‹œë§Œ)
            },
            ...
        ],
        "condition_points_earned": float, # ì¡°ê±´ìœ¼ë¡œ íšë“í•œ í•©ê³„(ë¶„ë¦¬ ê´€ë¦¬ ì‹œ ì°¸ê³ ìš©)
        "all_status": "success" | "fail", # ëª¨ë“  í•„ìˆ˜ ì¡°ê±´ í†µê³¼ ì—¬ë¶€(í•„ìˆ˜ ì—¬ë¶€ ì •ë³´ê°€ ì—†ìœ¼ë©´ 'ëª¨ë‘ pass' ê¸°ì¤€)
    }
    """

    def __init__(self, *, api_key: Optional[str] = None, model: Optional[str] = None, timeout: float = 30.0):
        self.client = AsyncOpenAI(api_key=api_key or OPENAI_API_KEY, timeout=timeout)
        self.model = model or OPENAI_MODEL

    # -----------------------------
    # Public: Dispatcher
    # -----------------------------
    async def generate_for_problem_type(
        self,
        *,
        problem_type: Literal["coding", "debugging", "multiple_choice", "short_answer", "subjective"],
        max_points: float,  # â† problem_ref.score / points (ìµœëŒ€ì ìˆ˜)
        # ê³µí†µ ì»¨í…ìŠ¤íŠ¸
        problem_description: str = "",
        # ì½”ë”©/ë””ë²„ê¹…
        code: str = "",
        language: str = "python",
        test_results: Optional[List[Dict[str, Any]]] = None,
        condition_check_results: Optional[List[Dict[str, Any]]] = None,
        # ì¡°ê±´ ì ìˆ˜ ì˜µì…˜(ì„ íƒ)
        points_for_conditions: float = 0.0,           # ì¡°ê±´ì— ë°°ë¶„í•  ì´ ë°°ì (ê¸°ë³¸ 0: ì¡°ê±´ ë“ì  ë¶„ë¦¬/ë¯¸ì‚¬ìš©)
        # ê°ê´€ì‹
        mc_correct_indices: Optional[List[int]] = None,
        mc_selected_indices: Optional[List[int]] = None,
        # ë‹¨ë‹µí˜•
        short_answer_text: Optional[str] = None,
        short_expected_answers: Optional[List[str]] = None,
        short_rating_mode: Literal["exact", "partial", "soft"] = "exact",
        # ì£¼ê´€ì‹
        subjective_text: Optional[str] = None,
        rubric: Optional[str] = None,
    ) -> Dict[str, Any]:

        condition_check_results = condition_check_results or []

        # 1) ìœ í˜•ë³„ ë°±ë¶„ìœ¨ ë° AI í”¼ë“œë°± ì‚°ì¶œ
        if problem_type in ("coding", "debugging"):
            percent = self._success_rate(test_results or [])
            ai_fb = await self._gen_feedback_coding_like(
                problem_description=problem_description,
                code=code,
                language=language,
                test_results=test_results or [],
                condition_check_results=condition_check_results,
                mode=("debugging" if problem_type == "debugging" else "coding"),
            )
            graded_by = "auto:testcase"

        elif problem_type == "multiple_choice":
            percent, exact = self._score_multiple_choice(
                correct=mc_correct_indices or [],
                selected=mc_selected_indices or [],
            )
            ai_fb = await self._gen_feedback_multiple_choice(
                problem_description=problem_description,
                correct=mc_correct_indices or [],
                selected=mc_selected_indices or [],
                exact_match=exact,
                percent=percent,
            )
            graded_by = "auto:exact"

        elif problem_type == "short_answer":
            # ì…ë ¥ ê°•ì œ ì •ê·œí™” (ë¦¬ìŠ¤íŠ¸/ë”•ì…”ë„ˆë¦¬ë„ ì•ˆì „)
            student_text_norm = self._normalize(short_answer_text)
            expected_list = short_expected_answers or []
            if not isinstance(expected_list, list):
                expected_list = [expected_list]
            expected_list = [self._normalize(x) for x in expected_list if self._normalize(x)]

            percent = self._score_short_answer(
                student_text=student_text_norm,
                expected=expected_list,
                mode=short_rating_mode,
            )
            ai_fb = await self._gen_feedback_short_answer(
                problem_description=problem_description,
                student_text=student_text_norm,
                expected=expected_list,
                mode=short_rating_mode,
                percent=percent,
            )
            graded_by = "auto:rule"

        else:  # subjective
            percent = await self._score_subjective_llm(subjective_text or "", rubric or "")
            ai_fb = await self._gen_feedback_subjective(
                problem_description=problem_description,
                text=subjective_text or "",
                rubric=rubric or "",
                percent=percent,
            )
            graded_by = "AI"

        # 2) ìŠ¤ì¼€ì¼ë§: 0~100 â†’ 0~max_points  (=> total_scoreì— í•´ë‹¹)
        score = round((float(percent) / 100.0) * float(max_points), 2)

        # 3) ì¡°ê±´ ê²°ê³¼(list[dict]) ê¸°ë°˜ í•©ê³„ ì ìˆ˜/ì „ì²´ í†µê³¼ ì—¬ë¶€ ê³„ì‚°
        cond_results = condition_check_results or []

        # ì¡°ê±´ë³„ ì ìˆ˜ í•©ê³„ (ì‚¬ì „ì— ë¶„ë°°ëœ score í•„ë“œë¥¼ í•©ì‚°)
        cond_sum = round(sum(float(it.get("score", 0.0)) for it in cond_results), 2)

        # í•„ìˆ˜ ì¡°ê±´(is_required=True) ì¤‘ í•˜ë‚˜ë¼ë„ ì‹¤íŒ¨í•˜ë©´ fail
        all_required_ok = all(
            (not bool(it.get("is_required", True))) or bool(it.get("passed", False))
            for it in cond_results
        )

        return {
            "percent": float(percent),
            "score": score,
            "ai_feedback": ai_fb,
            "graded_by": graded_by,
            "condition_results": cond_results,              # ê·¸ëŒ€ë¡œ ì €ì¥/ë°˜í™˜
            "condition_points_earned": cond_sum,           # ì¡°ê±´ ì ìˆ˜ í•©ê³„
            "all_status": "success" if all_required_ok else "fail",
            # "condition_overall_feedback" ì œê±° (ìš”ì•½ ë”•ì…”ë„ˆë¦¬ ì‚¬ìš© ì•ˆ í•¨)
        }

    # -----------------------------
    # Coding / Debugging
    # -----------------------------
    def _success_rate(self, test_results: List[Dict[str, Any]]) -> float:
        if not test_results:
            return 0.0
        total = len(test_results)
        passed = sum(1 for r in test_results if bool(r.get("passed")))
        return (passed / total) * 100.0

    async def _gen_feedback_coding_like(
        self,
        *,
        problem_description: str,
        code: str,
        language: str,
        test_results: List[Dict[str, Any]],
        condition_check_results: List[Dict[str, Any]],
        mode: Literal["coding", "debugging"] = "coding",
    ) -> str:
        failed: List[Dict[str, Any]] = []
        for idx, r in enumerate(test_results):
            if not r.get("passed"):
                failed.append({
                    "index": idx,
                    "input": r.get("input"),
                    "expected_output": r.get("expected_output"),
                    "output": r.get("output"),
                    "status": r.get("status"),
                    "error": r.get("error"),
                })
        failed = failed[:5]

        system = (
            "ë‹¹ì‹ ì€ í”„ë¡œê·¸ë˜ë° ê³¼ì œ í”¼ë“œë°± ì½”ì¹˜ì…ë‹ˆë‹¤.\n"
            "- ì¶œë ¥: í•œêµ­ì–´ ë¶ˆë¦¿ 3~5ì¤„, ê³¼ë„í•œ ì¥í™©í•¨ ê¸ˆì§€\n"
            "- í¬í•¨: (1)ì‹¤íŒ¨ ì›ì¸ ì¶”ì • (2)êµ¬ì²´ì  ìˆ˜ì • ì§€ì‹œ(ì¡°ê±´/ì¸ë±ìŠ¤/ìŠ¬ë¼ì´ìŠ¤/í¬ë§·/íŠ¸ë¦¬ë° ë“±)\n"
            "- ì´ì–´ì„œ 1~3ì¤„ ê·œëª¨ì˜ ```diff``` ë¯¸ë‹ˆ íŒ¨ì¹˜ ì œì‹œ(í•„ìš”ì‹œ)\n"
            "- ë§ˆì§€ë§‰ ì¤„ì€ ì—£ì§€ì¼€ì´ìŠ¤(ë¹ˆ/ëŒ€í˜• ì…ë ¥, ì¤‘ë³µ/ì •ë ¬, ê²½ê³„ê°’, íƒ€ì…/ì˜¤ë²„í”Œë¡œìš°) ì ê²€ ê¶Œê³ \n"
            "- ì ìˆ˜/JSON/ì „ì²´ í•´ë‹µ ê¸ˆì§€, ìµœì†Œ ë³€ê²½ë§Œ ì œì‹œ"
        )
        user_ctx = {
            "mode": mode,
            "problem": (problem_description or "")[:2000],
            "language": language,
            "failed_tests": failed,
            "condition_checks": condition_check_results[:10],
            "code_excerpt": (code[:1600] + "...(truncated)") if len(code) > 1600 else code,
        }
        user = (
            "ì•„ë˜ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ í”¼ë“œë°±ì„ ì‘ì„±í•˜ì„¸ìš”.\n\n"
            f"{json.dumps(user_ctx, ensure_ascii=False)}"
        )
        return await self._chat_once(system, user, max_tokens=500, temperature=0.4, fallback=self._fallback_coding_like(failed))

    def _fallback_coding_like(self, failed: List[Dict[str, Any]]) -> str:
        if failed:
            ids = ", ".join(str(f.get("index")) for f in failed if f.get("index") is not None)
            line1 = f"- ì‹¤íŒ¨ í…ŒìŠ¤íŠ¸ {len(failed)}ê±´: ì¼€ì´ìŠ¤ {ids} ì…ì¶œë ¥ í¬ë§·/ê°œí–‰/ê³µë°±/íŠ¸ë¦¬ë° ì ê²€."
        else:
            line1 = "- ì‹¤íŒ¨ í…ŒìŠ¤íŠ¸ ìš”ì•½ ì—†ìŒ: í‘œì¤€ì…ì¶œë ¥ í¬ë§·/ê°œí–‰/ê³µë°±/íŠ¸ë¦¬ë° ìš°ì„  ì ê²€."
        return "\n".join([
            line1,
            "- ìˆ˜ì • ì§€ì‹œ: ë¹„êµ ì—°ì‚°ì/ë£¨í”„ ìƒí•œ/ìŠ¬ë¼ì´ìŠ¤/ì¸ë±ìŠ¤ ë²”ìœ„/ì´ˆê¸°í™”/ë¦¬í„´ì„ ë¬¸ì œ ì •ì˜ì™€ ì¼ì¹˜.",
            "- ë¯¸ë‹ˆ íŒ¨ì¹˜ ì˜ˆ: ```diff\n- if (i <= n)\n+ if (i < n)\n```",
            "- ì—£ì§€: ë¹ˆ/ëŒ€í˜• ì…ë ¥, ì¤‘ë³µ/ì •ë ¬, ê²½ê³„ê°’, íƒ€ì… ìºìŠ¤íŒ…/ì˜¤ë²„í”Œë¡œìš° í™•ì¸."
        ])

    # -----------------------------
    # Multiple Choice
    # -----------------------------
    def _score_multiple_choice(self, *, correct: List[int], selected: List[int]) -> Tuple[float, bool]:
        c_set, s_set = set(correct), set(selected)
        exact = (c_set == s_set)
        if not correct:
            return 0.0, False
        return (100.0 if exact else 0.0), exact

    async def _gen_feedback_multiple_choice(
        self,
        *,
        problem_description: str,
        correct: List[int],
        selected: List[int],
        exact_match: bool,
        percent: float,
    ) -> str:
        sys = "ë‹¹ì‹ ì€ ì¹œê·¼í•˜ê³  ê²©ë ¤ì ì¸ í”„ë¡œê·¸ë˜ë° êµìœ¡ ë©˜í† ì…ë‹ˆë‹¤."
        user = f"""
ë¬¸ì œ ì„¤ëª…:
{(problem_description or '')[:1200]}

í•™ìƒì˜ ì„ íƒ: {selected}
ì •ë‹µ ê°œìˆ˜: {len(correct)}
ì±„ì  ê²°ê³¼: {percent:.1f}% ({'ì •í™•íˆ ì¼ì¹˜' if exact_match else 'ë¶ˆì¼ì¹˜'})

ìš”ì²­:
- ì§€ë‚˜ì¹œ ì •ë‹µ ëˆ„ì„¤ ì—†ì´, ì„ íƒ ì „ëµ/ì˜¤ë‹µ ìœ í˜•/ì§€ë¬¸ í•´ì„ íŒì„ 3~4ì¤„ë¡œ ì œì‹œ
- ê¸ì •ì ì¸ ë¶€ë¶„ 1ì¤„ + ê°œì„  ì œì•ˆ 2ì¤„ ì¤‘ì‹¬
- í•œêµ­ì–´, ì´ëª¨ì§€ 1~2ê°œ í—ˆìš©
"""
        return await self._chat_once(sys, user, max_tokens=300, temperature=0.6, fallback=self._fallback_mc(percent))

    def _fallback_mc(self, percent: float) -> str:
        if percent >= 100:
            return "ğŸ‰ ì •í™•í•˜ê²Œ ì„ íƒí–ˆì–´ìš”! í’€ì´ ê³¼ì •ì„ ìŠ¤ìŠ¤ë¡œ ì„¤ëª…í•´ë³´ë©´ ë” íƒ„íƒ„í•´ì ¸ìš”."
        return "âŒ ì •ë‹µê³¼ ë¶ˆì¼ì¹˜í–ˆì–´ìš”. ì§€ë¬¸ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œì™€ ë°°ì œ ê·¼ê±°ë¥¼ ë¨¼ì € í‘œì‹í•œ ë’¤, ë‚¨ì€ ë³´ê¸°ì˜ ì°¨ì´ë¥¼ ë¹„êµí•´ ë³´ì„¸ìš”."

    # -----------------------------
    # Short Answer
    # -----------------------------
    def _score_short_answer(self, *, student_text: str, expected: List[str], mode: str) -> float:
        s = self._normalize(student_text)
        if not expected:
            return 0.0
        def exact_ok(ans: str) -> bool:
            return s == self._normalize(ans)
        def partial_ok(ans: str) -> bool:
            a, b = s.lower(), self._normalize(ans).lower()
            return (a in b) or (b in a)
        def soft_ok(ans: str) -> bool:
            import difflib
            a, b = s.lower(), self._normalize(ans).lower()
            return difflib.SequenceMatcher(None, a, b).ratio() >= 0.8
        matcher = {"exact": exact_ok, "partial": partial_ok, "soft": soft_ok}.get(mode, exact_ok)
        ok = any(matcher(ans) for ans in expected)
        return 100.0 if ok else 0.0

    async def _gen_feedback_short_answer(
        self,
        *,
        problem_description: str,
        student_text: str,
        expected: List[str],
        mode: str,
        percent: float,
    ) -> str:
        sys = "ë‹¹ì‹ ì€ ê°„ê²°ì„±ê³¼ ì •í™•ì„±ì„ ì¤‘ì‹œí•˜ëŠ” í‰ê°€ìì…ë‹ˆë‹¤."
        expected_hint = expected[0] if expected else ""
        student_view = self._normalize(student_text)  # ì—¬ê¸°ì„œ ì •ê·œí™”

        user = f"""
    ë¬¸ì œ:
    {(problem_description or '')[:1200]}

    í•™ìƒ ë‹µì•ˆ: {student_view or '(ë¹ˆ ì…ë ¥)'}
    ì±„ì  ëª¨ë“œ: {mode}
    ê²°ê³¼: {percent:.1f}%

    ìš”ì²­:
    - 3~4ì¤„: (1)í•µì‹¬ ê°œë… ì¼ì¹˜/ë¶ˆì¼ì¹˜ (2)í‘œí˜„ì˜ ëª¨í˜¸ì„±/ë‹¨ìœ„/í¬ë§· (3)ì¶”ê°€ í™•ì¸ í¬ì¸íŠ¸
    - ì •ë‹µ ì „ì²´ë¥¼ ê·¸ëŒ€ë¡œ ë…¸ì¶œí•˜ì§€ ë§ê³ , ëŒ€í‘œ ì •ë‹µ í˜•íƒœë§Œ íŒíŠ¸ ìˆ˜ì¤€ìœ¼ë¡œ ì–¸ê¸‰
    - í•œêµ­ì–´, ì´ëª¨ì§€ 1ê°œ ì´í•˜
    íŒíŠ¸ ì˜ˆì‹œ(ëŒ€í‘œ): {expected_hint}
    """
        return await self._chat_once(sys, user, max_tokens=280, temperature=0.5, fallback=self._fallback_short(percent))

    def _fallback_short(self, percent: float) -> str:
        return "ğŸ‘ í•µì‹¬ ìš©ì–´ì™€ í¬ë§·ì„ ë‹¤ì‹œ í•œ ë²ˆ ì •ëˆí•´ ë³´ì„¸ìš”. ë‹¨ìœ„/ì² ì/ê³µë°±ì„ ì •í™•íˆ ë§ì¶”ë©´ ì •ë‹µì— ë” ê°€ê¹Œì›Œì§‘ë‹ˆë‹¤." if percent < 100 else "ğŸ‰ ì •í™•í•©ë‹ˆë‹¤! ê°™ì€ ê°œë…ì„ ë‹¤ë¥¸ í‘œí˜„ìœ¼ë¡œë„ ì„¤ëª…í•´ ë³´ì„¸ìš”."

    # -----------------------------
    # Subjective (Essay)
    # -----------------------------
    async def _score_subjective_llm(self, text: str, rubric: str) -> float:
        if not text.strip():
            return 0.0

        try:
            resp = await self.client.responses.create(
                model=self.model,
                input=[
                    {"role": "system", "content": self._system_prompt_subjective(rubric)},
                    {"role": "user", "content": f"Answer:\n{text.strip()[:6000]}"},
                ],
                response_format={
                    "type": "json_schema",
                    "json_schema": {
                        "name": "subjective_score",
                        "schema": {
                            "type": "object",
                            "properties": {"score": {"type": "number", "minimum": 0, "maximum": 100}},
                            "required": ["score"],
                            "additionalProperties": False,
                        },
                        "strict": True,
                    },
                },
                max_output_tokens=40,
            )
            val = None
            if hasattr(resp, "output_parsed") and resp.output_parsed:
                val = resp.output_parsed
            elif hasattr(resp, "output") and resp.output:
                for it in resp.output:
                    if getattr(it, "type", "") == "parsed_json":
                        val = getattr(it, "parsed", None)
                        break
            if isinstance(val, dict) and "score" in val:
                return float(max(0.0, min(100.0, float(val["score"]))))

            text_out = getattr(resp, "output_text", None) or str(resp)
            return self._parse_first_number(text_out)

        except Exception:
            pass

        try:
            prompt = (
                "ë‹¤ìŒ ë‹µì•ˆì„ 0~100ìœ¼ë¡œ ì±„ì í•˜ì„¸ìš”. ë°˜ë“œì‹œ ë‹¤ìŒ JSONë§Œ ì¶œë ¥: "
                '{"score": <0~100 ìˆ«ì>}\n\n'
                f"[ë£¨ë¸Œë¦­]\n{rubric or 'ì¼ë°˜ ì›ì¹™ ì ìš©'}\n\n[ë‹µì•ˆ]\n{text.strip()[:6000]}"
            )
            cc = await self.client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.0,
                max_tokens=40,
            )
            content = cc.choices[0].message.content or ""
            return self._parse_first_number(content)
        except Exception:
            return 0.0

    async def _gen_feedback_subjective(self, *, problem_description: str, text: str, rubric: str, percent: float) -> str:
        sys = "ë‹¹ì‹ ì€ ê³µì •í•˜ê³  ì—„ê²©í•˜ì§€ë§Œ í•™ìƒì„ ì¡´ì¤‘í•˜ëŠ” ì—ì„¸ì´ í‰ê°€ìì…ë‹ˆë‹¤."
        user = f"""
ë¬¸ì œ/ì£¼ì œ:
{(problem_description or '')[:1200]}

ë£¨ë¸Œë¦­:
{rubric or 'ì¼ë°˜ ì›ì¹™ ì ìš©'}

í•™ìƒ ë‹µì•ˆ(ìš”ì•½ í‰ê°€):
{text.strip()[:2000]}

ì ìˆ˜(ë°±ë¶„ìœ¨): {percent:.1f}%

ìš”ì²­:
- 4~5ì¤„: (1)í•µì‹¬ ì£¼ì¥/ê·¼ê±° í‰ê°€ (2)ë…¼ë¦¬ ì „ê°œ/êµ¬ì¡° (3)ì‚¬ì‹¤ì„±Â·ê·¼ê±°ì„± (4)ê°œì„  ì œì•ˆ
- ê³¼ë„í•œ ì•„ë¶€/ê³µí—ˆí•œ ì¹­ì°¬ ê¸ˆì§€, êµ¬ì²´ì  ë¬¸ì¥ìœ¼ë¡œ
"""
        return await self._chat_once(sys, user, max_tokens=360, temperature=0.4, fallback=self._fallback_subjective(percent))

    def _system_prompt_subjective(self, rubric: str) -> str:
        return (
            "You are a strict but fair grader. Return ONLY JSON per schema with a single numeric field 'score' (0~100). "
            "No explanations."
            f"\nRubric:\n{rubric or 'Use general principles.'}"
        )

    def _fallback_subjective(self, percent: float) -> str:
        if percent >= 90:
            return "í•µì‹¬ ì£¼ì¥ê³¼ ê·¼ê±°ê°€ ëšœë ·í•©ë‹ˆë‹¤. ê²°ë¡ ë¶€ì—ì„œ í•¨ì˜ë¥¼ í•œ ì¤„ ë” í™•ì¥í•˜ë©´ ì™„ì„±ë„ê°€ ë†’ì•„ì§‘ë‹ˆë‹¤."
        if percent >= 70:
            return "ì£¼ì¥ì˜ ë°©í–¥ì€ ë§ì§€ë§Œ ê·¼ê±°ê°€ ì–•ìŠµë‹ˆë‹¤. êµ¬ì²´ ì‚¬ë¡€/ë°ì´í„°ë¥¼ 1~2ê°œ ë³´ê°•í•´ ë³´ì„¸ìš”."
        return "í•µì‹¬ ë…¼ì§€ë¥¼ í•œ ì¤„ë¡œ ì •ë¦¬í•˜ê³ , ê·¸ì— ë§ëŠ” ê·¼ê±°â†’ë¶„ì„â†’ê²°ë¡ ì˜ êµ¬ì¡°ë¥¼ ê°„ê²°í•˜ê²Œ ì¬ë°°ì¹˜í•´ ë³´ì„¸ìš”."

    # -----------------------------
    # Low-level chat helper
    # -----------------------------
    async def _chat_once(self, system: str, user: str, *, max_tokens: int, temperature: float, fallback: str) -> str:
        try:
            resp = await self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": system},
                    {"role": "user", "content": user},
                ],
                temperature=temperature,
                max_tokens=max_tokens,
            )
            return (resp.choices[0].message.content or "").strip()
        except (RateLimitError, APITimeoutError, APIError, Exception):
            return fallback

    # -----------------------------
    # Formatting helpers (ì¬ì‚¬ìš©/ë””ë²„ê¹…ìš©)
    # -----------------------------
    def _format_test_results(self, test_results: List[Dict[str, Any]]) -> str:
        if not test_results:
            return "í…ŒìŠ¤íŠ¸ ê²°ê³¼ ì—†ìŒ"
        passed_count = sum(1 for r in test_results if r.get("passed"))
        total_count = len(test_results)
        lines = [f"ì´ {total_count}ê°œ í…ŒìŠ¤íŠ¸ ì¤‘ {passed_count}ê°œ í†µê³¼ ({passed_count/total_count*100:.1f}%)", ""]
        for i, r in enumerate(test_results, 1):
            status = "âœ… í†µê³¼" if r.get("passed") else "âŒ ì‹¤íŒ¨"
            lines.append(f"í…ŒìŠ¤íŠ¸ {i}: {status}")
            if not r.get("passed"):
                lines.append(f"  - ì…ë ¥: {r.get('input', 'N/A')}")
                lines.append(f"  - ì˜ˆìƒ: {r.get('expected_output', 'N/A')}")
                lines.append(f"  - ì‹¤ì œ: {r.get('output', 'N/A')}")
                if r.get("error"):
                    lines.append(f"  - ì˜¤ë¥˜: {r.get('error')}")
            lines.append("")
        return "\n".join(lines)

    def _format_condition_results(self, condition_results: List[Dict[str, Any]]) -> str:
        if not condition_results:
            return "ì¡°ê±´ ì²´í¬ ê²°ê³¼ ì—†ìŒ"
        out = [f"ì´ {len(condition_results)}ê°œ ì¡°ê±´ ì²´í¬", ""]
        for c in condition_results:
            status = "âœ… ì¶©ì¡±" if c.get("status") == "pass" or c.get("passed") else "âŒ ë¯¸ì¶©ì¡±"
            conf = f"({c.get('confidence', 0):.1f})" if c.get("confidence") else ""
            name = c.get("condition") or c.get("description") or "Unknown"
            fb = c.get("feedback", "No feedback")
            out.append(f"{name}: {status} {conf}")
            out.append(f"  - í”¼ë“œë°±: {fb}")
            out.append("")
        return "\n".join(out)

    # -----------------------------
    # Condition helpers
    # -----------------------------
    def _distribute_points(self, n: int, total: float) -> List[float]:
        if n <= 0:
            return []
        per = round(total / n, 2)
        scores = [per] * (n - 1)
        tail = round(total - sum(scores), 2)
        scores.append(tail)
        return scores

    def _build_condition_results(
        self,
        *,
        checks: List[Dict[str, Any]] | List[object],
        total_points_for_conditions: float = 0.0,
    ) -> Tuple[List[Dict[str, Any]], float, Literal["success", "fail"]]:
        """
        ë‹¤ì–‘í•œ í˜•íƒœ(checks[i].passed / checks[i]['passed'] â€¦)ë¥¼ í¡ìˆ˜í•˜ì—¬
        ConditionResult JSON ë¦¬ìŠ¤íŠ¸, ì¡°ê±´ ë“ì  í•©ê³„, ì „ì²´ í†µê³¼ ì—¬ë¶€ë¥¼ ë°˜í™˜.
        í•„ìˆ˜ì—¬ë¶€(is_required)ê°€ ì£¼ì–´ì§€ì§€ ì•Šìœ¼ë©´ ëª¨ë‘ í•„ìˆ˜ë¡œ ê°„ì£¼.
        """
        checks = checks or []
        count = len(checks)
        per_scores = self._distribute_points(count, float(total_points_for_conditions)) if count else []
        results: List[Dict[str, Any]] = []
        earned_sum = 0.0
        passed_all_required = True

        for i, c in enumerate(checks, 1):
            # ì†ì„±/í‚¤ë¥¼ ëª¨ë‘ í—ˆìš©
            get = (lambda k, d=None: (getattr(c, k, d) if not isinstance(c, dict) else c.get(k, d)))
            passed = bool(get("passed", False))
            condition_text = str(get("condition", "")) or str(get("description", "")) or ""
            description = str(get("description", "")) or condition_text
            feedback = str(get("feedback", "")) if get("feedback", "") is not None else ""
            is_required = bool(get("is_required", True))  # ì •ë³´ ì—†ìœ¼ë©´ í•„ìˆ˜ë¡œ ê°„ì£¼

            per = float(per_scores[i - 1]) if i - 1 < len(per_scores) else 0.0
            got = per if passed else 0.0
            earned_sum += got

            if is_required and not passed:
                passed_all_required = False

            results.append({
                "condition_id": i,
                "condition": condition_text,
                "status": "pass" if passed else "fail",
                "description": description,
                "feedback": feedback,
                "score": round(got, 2),
            })

        all_status: Literal["success", "fail"] = "success" if (passed_all_required if results else True) else "fail"
        return results, round(earned_sum, 2), all_status

    # -----------------------------
    # Utils
    # -----------------------------
    def _normalize(self, s: object) -> str:
        if s is None:
            return ""
        if isinstance(s, (list, tuple, set)):
            try:
                s = " ".join(map(str, s))
            except Exception:
                s = " ".join([str(x) for x in list(s)])
        elif isinstance(s, dict):
            try:
                s = json.dumps(s, ensure_ascii=False, separators=(",", ":"))
            except Exception:
                s = str(s)
        elif not isinstance(s, str):
            s = str(s)
        s = s.strip()
        s = re.sub(r"\s+", " ", s)
        return s

    def _parse_first_number(self, s: str) -> float:
        if not s:
            return 0.0
        m = re.search(r"(\d+(?:\.\d+)?)", s)
        if not m:
            return 0.0
        v = float(m.group(1))
        return max(0.0, min(100.0, v))
